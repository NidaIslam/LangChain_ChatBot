{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain ChatBot - Chat with Multiple Documents ðŸ¤–ðŸ“š\n",
        "\n",
        "âœ¨This project introduces an AI-powered chatbot designed to interact with multiple uploaded documents (PDFs,texts, Word files, presentations). Leveraging the Retrieval-Augmented Generation (RAG) approach, the chatbot efficiently combines document retrieval with language generation, providing accurate and contextually relevant answers.\n",
        "\n",
        "ðŸš€**Key Features:**\n",
        "\n",
        "* **Multi-Document Support:**  Upload and query information from multiple PDF, Word, and presentation files.\n",
        "* **Retrieval-Augmented Generation (RAG):** Utilizes external knowledge sources for enhanced accuracy and efficiency.\n",
        "* **Contextual Understanding:**  Provides answers tailored to the specific context of your uploaded documents.\n",
        "* **Streamlit-Powered Interface:**  Offers a user-friendly web interface for interaction.\n",
        "\n",
        "âœ…**Benefits:**\n",
        "\n",
        "* **Streamlined Document Interaction:** Simplifies document analysis and knowledge extraction.\n",
        "* **Scalability and Efficiency:**  Scales effortlessly without extensive model retraining.\n",
        "* **Improved Accuracy:** Provides contextually relevant and accurate responses.\n",
        "\n",
        "This solution empowers users to effectively interact with and extract insights from their documents, enhancing productivity and streamlining information retrieval."
      ],
      "metadata": {
        "id": "HsM8gE0Ol54L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "YpQ52yVCkN8_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "fi4iUpW6XMvy"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit -q\n",
        "!pip install langchain -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install tiktoken -q\n",
        "!pip install huggingface_hub -q\n",
        "!pip install python-dotenv -q\n",
        "!pip install -U langchain-community -q\n",
        "!pip install -U sentence-transformers -q\n",
        "!pip install python-docx -q\n",
        "!pip install python-pptx -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Application"
      ],
      "metadata": {
        "id": "Z2HL5A1OMMIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace API Setup and Configuration"
      ],
      "metadata": {
        "id": "OEoWyQeDnzrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Securely take the API token as input without showing the characters\n",
        "huggingface_api_key = getpass.getpass(\"Enter your HuggingFace API key:\")\n",
        "\n",
        "if huggingface_api_key:\n",
        "    # Set the HuggingFace API token as an environment variable\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_api_key\n",
        "    print(\"API token securely set!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhglTNGuRlp1",
        "outputId": "a28df948-b9ad-4989-aacc-dc80d7d78056"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your HuggingFace API key:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "API token securely set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit App"
      ],
      "metadata": {
        "id": "atbLsVwkn8SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "\n",
        "CSS = '''\n",
        "<style>\n",
        ".chat-message {\n",
        "padding: 1.5rem; border-radius: 0.5rem; margin-bottom:1rem; display: flex\n",
        "}\n",
        ".chat-message.user{\n",
        "    background-color: #2b313e\n",
        "}\n",
        ".chat-message.bot{\n",
        "    background-color: #233861\n",
        "}\n",
        ".chat-message .avatar {\n",
        "width: 15%;\n",
        "}\n",
        ".chat-message .avatar img {\n",
        "max-width: 78px;\n",
        "max-height: 78px;\n",
        "border-radius: 50%;\n",
        "object-fit: cover;\n",
        "}\n",
        ".chat-message .message {\n",
        "width: 85%;\n",
        "padding: 0 1.5rem;\n",
        "color: #fff;\n",
        "}\n",
        "'''\n",
        "bot_template = '''\n",
        "<div class=\"chat-message bot\">\n",
        "    <div class=\"avatar\">\n",
        "        <img src=\"https://cbx-prod.b-cdn.net/COLOURBOX30822090.jpg?width=800&height=800&quality=70\" style=\"max-height: 50px; max-width: 50px; border-radius: 10%; object-fit: cover;\">\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "user_template = '''\n",
        "<div class=\"chat-message user\">\n",
        "    <div class=\"avatar\">\n",
        "        <img src=\"https://cbx-prod.b-cdn.net/COLOURBOX3058324.jpg?width=800&height=800&quality=70\" style=\"max-height: 50px; max-width: 50px; border-radius: 10%; object-fit: cover;\">\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "\n",
        "def get_file_text(uploaded_files):\n",
        "    \"\"\"\n",
        "    Extracts text from PDF, TXT, Word (DOCX), and PowerPoint (PPTX) files.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        for uploaded_file in uploaded_files:\n",
        "            file_name = uploaded_file.name.lower()\n",
        "\n",
        "            # Check if file is a PDF\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                pdf_reader = PdfReader(uploaded_file)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() or \"\"\n",
        "\n",
        "            # Check if file is a TXT file\n",
        "            elif file_name.endswith(\".txt\"):\n",
        "                text += uploaded_file.read().decode(\"utf-8\")\n",
        "\n",
        "            # Check if file is a Word document\n",
        "            elif file_name.endswith(\".docx\"):\n",
        "                doc = Document(uploaded_file)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + \"\\n\"\n",
        "\n",
        "            # Check if file is a PowerPoint presentation\n",
        "            elif file_name.endswith(\".pptx\"):\n",
        "                ppt = Presentation(uploaded_file)\n",
        "                for slide in ppt.slides:\n",
        "                    for shape in slide.shapes:\n",
        "                        if hasattr(shape, \"text\"):\n",
        "                            text += shape.text + \"\\n\"\n",
        "\n",
        "            # Unsupported file type\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file type: {file_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading files: {e}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"\n",
        "    Splits text into smaller chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector store using HuggingFace embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the HuggingFace embedding model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Create the FAISS vector store\n",
        "    vectorstore = FAISS.from_texts(text_chunks, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    \"\"\"\n",
        "    Initializes a conversational chain with a memory buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
        "        model_kwargs={\n",
        "            \"temperature\": 0.4,\n",
        "            \"max_length\": 500,\n",
        "            \"top_p\": 0.8,\n",
        "            \"repetition_penalty\": 1.2\n",
        "        }\n",
        "    )\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "    conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversational_chain\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    \"\"\"\n",
        "    Handles user input and displays conversation.\n",
        "    \"\"\"\n",
        "    if not st.session_state.conversation:\n",
        "        st.error(\"Please upload and process your documents first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        response = st.session_state.conversation({'question': user_question})\n",
        "        st.session_state.chat_history = response['chat_history']\n",
        "        for i, message in enumerate(st.session_state.chat_history):\n",
        "            if i % 2 == 0:\n",
        "                st.write(user_template.replace(\"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "            else:\n",
        "                st.write(bot_template.replace(\"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing your question: {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main Streamlit app function.\n",
        "    \"\"\"\n",
        "\n",
        "    load_dotenv()\n",
        "    st.set_page_config(page_title=\"Chat with multiple Docs\", page_icon=\":books:\")\n",
        "    st.write(CSS, unsafe_allow_html=True)\n",
        "\n",
        "    if \"conversation\" not in st.session_state:\n",
        "        st.session_state.conversation = None\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    st.header(\"Chat with multiple Documents :robot_face::books:\")\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.subheader(\":open_file_folder: Your documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your Documents here and click 'Process'\", accept_multiple_files=True)\n",
        "        if st.button(\"Process\"):\n",
        "            if pdf_docs:\n",
        "                with st.spinner(\"Processing...\"):\n",
        "                    raw_text = get_file_text(pdf_docs)\n",
        "                    text_chunks = get_text_chunks(raw_text)\n",
        "                    vectorstore = get_vectorstore(text_chunks)\n",
        "                    st.session_state.conversation = get_conversation_chain(vectorstore)\n",
        "                st.success(\"Documents processed successfully!\")\n",
        "            else:\n",
        "                st.error(\"Please upload at least one File.\")\n",
        "\n",
        "    user_question = st.text_input(\"Ask a question about your document: :mag:\")\n",
        "    if user_question:\n",
        "        handle_userinput(user_question)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKBZT1-v1dhq",
        "outputId": "26c457fd-e460-414a-b0a0-29576633d7e2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install localtunnel to run the Streamlit Application on Web\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_jExI0woH1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4FL7txsMyLR",
        "outputId": "8565fcca-65e9-4442-8454-e37444c757c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "id": "wSZUJlQFMyHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5oqxIVp0LnZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}